{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Feature Visualization with CRP\n",
    "\n",
    "This notebook demonstrates advanced feature visualization techniques for time series models using CRP.\n",
    "We'll explore concept maximization, receptive field analysis, and statistical analysis of learned features.\n",
    "\n",
    "## Overview\n",
    "- **Concept Maximization**: Find samples that maximally activate specific concepts\n",
    "- **Statistical Analysis**: Understand concept behavior across different classes\n",
    "- **Receptive Field Analysis**: Visualize temporal receptive fields\n",
    "- **Layer-wise Analysis**: Compare concepts across different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Add the CRP modules to path\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import CRP components\n",
    "from crp.attribution import CondAttribution\n",
    "from crp.helper import get_layer_names, get_output_shapes\n",
    "from crp.graph import trace_model_graph\n",
    "\n",
    "# Import adapted components\n",
    "from time_series_concepts import TimeSeriesConcept\n",
    "from time_series_visualization import (\n",
    "    plot_time_series_with_relevance,\n",
    "    plot_channel_importance,\n",
    "    visualize_temporal_concepts\n",
    ")\n",
    "from time_series_feature_visualization import TimeSeriesFeatureVisualization\n",
    "\n",
    "# Import model and dataset\n",
    "from cnn1d_model import CNN1D_Wide, VibrationDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Zennit composites\n",
    "from zennit.composites import EpsilonGammaBox, EpsilonPlus\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = CNN1D_Wide()\n",
    "model.load_state_dict(torch.load('../cnn1d_model.ckpt', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_dir = \"../data/final/new_selection/normalized_windowed_downsampled_data\"\n",
    "full_dataset = VibrationDataset(data_dir)\n",
    "\n",
    "# Create a smaller subset for feature visualization (computational efficiency)\n",
    "subset_size = 1000  # Use 1000 samples for analysis\n",
    "indices = np.random.choice(len(full_dataset), subset_size, replace=False)\n",
    "dataset = Subset(full_dataset, indices)\n",
    "\n",
    "print(f\"Full dataset size: {len(full_dataset)}\")\n",
    "print(f\"Analysis subset size: {len(dataset)}\")\n",
    "\n",
    "# Check class distribution in subset\n",
    "subset_labels = [full_dataset.labels[i] for i in indices]\n",
    "print(f\"Subset - Good: {sum(l == 0 for l in subset_labels)}, Bad: {sum(l == 1 for l in subset_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Feature Visualization Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get layer information\n",
    "conv_layers = get_layer_names(model, [nn.Conv1d])\n",
    "linear_layers = get_layer_names(model, [nn.Linear])\n",
    "all_layers = conv_layers + linear_layers\n",
    "\n",
    "print(\"Available layers:\")\n",
    "for i, layer in enumerate(all_layers):\n",
    "    print(f\"  {i}: {layer}\")\n",
    "\n",
    "# Get output shapes for each layer\n",
    "sample_data, _ = dataset[0]\n",
    "sample_input = sample_data.unsqueeze(0).to(device)\n",
    "output_shapes = get_output_shapes(model, sample_input, all_layers)\n",
    "\n",
    "print(\"\\nLayer output shapes:\")\n",
    "for layer, shape in output_shapes.items():\n",
    "    print(f\"  {layer}: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layer mapping and attribution objects\n",
    "layer_map = {name: TimeSeriesConcept() for name in all_layers}\n",
    "attribution = CondAttribution(model, device)\n",
    "\n",
    "# Create feature visualization object\n",
    "fv = TimeSeriesFeatureVisualization(\n",
    "    attribution=attribution,\n",
    "    dataset=dataset,\n",
    "    layer_map=layer_map,\n",
    "    preprocess_fn=None,  # Add your preprocessing if needed\n",
    "    device=device,\n",
    "    path=\"FeatureVisualization_TimeSeries\"\n",
    ")\n",
    "\n",
    "print(\"Feature visualization framework initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Feature Visualization Analysis\n",
    "\n",
    "This will analyze the dataset to find samples that maximally activate different concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a composite for analysis\n",
    "composite = EpsilonGammaBox(low=-1, high=1, gamma=0.25)\n",
    "\n",
    "# Run feature visualization on a subset (this can take time)\n",
    "print(\"Running feature visualization analysis...\")\n",
    "print(\"This may take several minutes depending on your hardware.\")\n",
    "\n",
    "# Analyze first 200 samples to keep computation manageable\n",
    "analysis_size = 200\n",
    "saved_files = fv.run(\n",
    "    composite=composite,\n",
    "    data_start=0,\n",
    "    data_end=analysis_size,\n",
    "    batch_size=16,\n",
    "    checkpoint=50  # Save checkpoints every 50 samples\n",
    ")\n",
    "\n",
    "print(\"Feature visualization analysis complete!\")\n",
    "print(f\"Saved files: {saved_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Concept Maximization Analysis\n",
    "\n",
    "Find samples that maximally activate specific concepts in different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the first convolutional layer\n",
    "layer_to_analyze = conv_layers[0]\n",
    "print(f\"Analyzing layer: {layer_to_analyze}\")\n",
    "print(f\"Layer output shape: {output_shapes[layer_to_analyze]}\")\n",
    "\n",
    "# Get the number of concepts (channels) in this layer\n",
    "num_concepts = output_shapes[layer_to_analyze][0]  # First dimension is channels\n",
    "print(f\"Number of concepts in {layer_to_analyze}: {num_concepts}\")\n",
    "\n",
    "# Analyze first 6 concepts\n",
    "concepts_to_analyze = list(range(min(6, num_concepts)))\n",
    "print(f\"Analyzing concepts: {concepts_to_analyze}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximally activating samples for each concept\n",
    "max_samples = fv.get_max_reference(\n",
    "    concept_ids=concepts_to_analyze,\n",
    "    layer_name=layer_to_analyze,\n",
    "    mode=\"activation\",  # Use activation maximization\n",
    "    r_range=(0, 5),     # Get top 5 samples\n",
    "    composite=composite,\n",
    "    plot_fn=None,       # We'll create our own plots\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(f\"Retrieved maximizing samples for {len(max_samples)} concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize maximally activating samples\n",
    "def plot_concept_maximizing_samples(samples_dict, layer_name, max_samples_to_show=3):\n",
    "    \"\"\"Plot samples that maximally activate each concept\"\"\"\n",
    "    n_concepts = len(samples_dict)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_concepts, max_samples_to_show, \n",
    "                            figsize=(4*max_samples_to_show, 3*n_concepts))\n",
    "    \n",
    "    if n_concepts == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    channel_names = ['X', 'Y', 'Z']\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for concept_idx, (concept_id, (samples, heatmaps)) in enumerate(samples_dict.items()):\n",
    "        for sample_idx in range(min(max_samples_to_show, len(samples))):\n",
    "            ax = axes[concept_idx, sample_idx]\n",
    "            \n",
    "            # Plot time series\n",
    "            sample = samples[sample_idx]\n",
    "            for ch in range(3):\n",
    "                ax.plot(sample[ch, :].numpy(), color=colors[ch], \n",
    "                       alpha=0.7, linewidth=1, label=channel_names[ch] if sample_idx == 0 else \"\")\n",
    "            \n",
    "            ax.set_title(f'Concept {concept_id}\\nSample {sample_idx+1}')\n",
    "            if sample_idx == 0:\n",
    "                ax.set_ylabel(f'Concept {concept_id}')\n",
    "                ax.legend(fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xlabel('Time Steps')\n",
    "    \n",
    "    plt.suptitle(f'Maximally Activating Samples - {layer_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot the results\n",
    "fig = plot_concept_maximizing_samples(max_samples, layer_to_analyze)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis of Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze concept statistics across different classes\n",
    "print(\"Computing concept statistics...\")\n",
    "\n",
    "concept_stats = {}\n",
    "for concept_id in concepts_to_analyze:\n",
    "    try:\n",
    "        sorted_targets, sorted_values = fv.compute_stats(\n",
    "            concept_id=concept_id,\n",
    "            layer_name=layer_to_analyze,\n",
    "            mode=\"relevance\",\n",
    "            top_N=2,  # Only 2 classes (good/bad)\n",
    "            mean_N=5,\n",
    "            norm=True\n",
    "        )\n",
    "        concept_stats[concept_id] = (sorted_targets, sorted_values)\n",
    "        print(f\"Concept {concept_id}: Targets {sorted_targets}, Values {sorted_values.numpy()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute stats for concept {concept_id}: {e}\")\n",
    "\n",
    "print(f\"Computed statistics for {len(concept_stats)} concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize concept-class relationships\n",
    "if len(concept_stats) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (concept_id, (targets, values)) in enumerate(concept_stats.items()):\n",
    "        if i >= 6:  # Only plot first 6 concepts\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Convert targets to class names\n",
    "        class_names = ['Good' if t == 0 else 'Bad' for t in targets]\n",
    "        colors_bar = ['green' if t == 0 else 'red' for t in targets]\n",
    "        \n",
    "        bars = ax.bar(class_names, values.numpy(), color=colors_bar, alpha=0.7)\n",
    "        ax.set_title(f'Concept {concept_id} - Class Preference')\n",
    "        ax.set_ylabel('Normalized Relevance')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars, values.numpy()):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(concept_stats), 6):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'Concept-Class Relationships - {layer_to_analyze}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"No concept statistics available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Receptive Field Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze receptive fields for concepts\n",
    "# Get relevance-based samples to understand temporal receptive fields\n",
    "rel_samples = fv.get_max_reference(\n",
    "    concept_ids=concepts_to_analyze[:3],  # Analyze first 3 concepts\n",
    "    layer_name=layer_to_analyze,\n",
    "    mode=\"relevance\",\n",
    "    r_range=(0, 3),  # Top 3 samples\n",
    "    composite=composite,\n",
    "    rf=True,  # Enable receptive field analysis\n",
    "    plot_fn=None,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(f\"Retrieved relevance-based samples with RF analysis for {len(rel_samples)} concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize receptive field patterns\n",
    "def plot_receptive_field_analysis(samples_dict, layer_name):\n",
    "    \"\"\"Plot receptive field analysis for concepts\"\"\"\n",
    "    n_concepts = len(samples_dict)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_concepts, 2, figsize=(12, 4*n_concepts))\n",
    "    if n_concepts == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    channel_names = ['X', 'Y', 'Z']\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for concept_idx, (concept_id, (samples, heatmaps)) in enumerate(samples_dict.items()):\n",
    "        # Plot first sample with its heatmap\n",
    "        sample = samples[0]\n",
    "        heatmap = heatmaps[0]\n",
    "        \n",
    "        # Time series plot\n",
    "        ax1 = axes[concept_idx, 0]\n",
    "        for ch in range(3):\n",
    "            ax1.plot(sample[ch, :].numpy(), color=colors[ch], \n",
    "                    alpha=0.7, label=channel_names[ch])\n",
    "        ax1.set_title(f'Concept {concept_id} - Time Series')\n",
    "        ax1.set_ylabel('Amplitude')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Relevance heatmap\n",
    "        ax2 = axes[concept_idx, 1]\n",
    "        ax2.fill_between(range(len(heatmap)), 0, heatmap.numpy(), \n",
    "                        alpha=0.7, color='red', label='Relevance')\n",
    "        ax2.set_title(f'Concept {concept_id} - Receptive Field')\n",
    "        ax2.set_ylabel('Relevance')\n",
    "        ax2.set_xlabel('Time Steps')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Find peak relevance region\n",
    "        peak_idx = torch.argmax(torch.abs(heatmap))\n",
    "        ax2.axvline(peak_idx, color='black', linestyle='--', alpha=0.7, \n",
    "                   label=f'Peak at {peak_idx}')\n",
    "        ax2.legend()\n",
    "    \n",
    "    plt.suptitle(f'Receptive Field Analysis - {layer_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot receptive field analysis\n",
    "if len(rel_samples) > 0:\n",
    "    fig = plot_receptive_field_analysis(rel_samples, layer_to_analyze)\n",
    "    plt.show()\nelse:\n",
    "    print(\"No relevance samples available for RF analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Layer Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare concepts across different layers\n",
    "layers_to_compare = conv_layers[:3]  # First 3 conv layers\n",
    "print(f\"Comparing layers: {layers_to_compare}\")\n",
    "\n",
    "layer_activations = {}\n",
    "for layer in layers_to_compare:\n",
    "    try:\n",
    "        # Get samples for first concept in each layer\n",
    "        samples = fv.get_max_reference(\n",
    "            concept_ids=[0],  # First concept\n",
    "            layer_name=layer,\n",
    "            mode=\"activation\",\n",
    "            r_range=(0, 2),  # Top 2 samples\n",
    "            composite=composite,\n",
    "            plot_fn=None,\n",
    "            batch_size=4\n",
    "        )\n",
    "        layer_activations[layer] = samples\n",
    "        print(f\"Retrieved samples for {layer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not analyze {layer}: {e}\")\n",
    "\n",
    "print(f\"Successfully analyzed {len(layer_activations)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-layer comparison\n",
    "if len(layer_activations) > 0:\n",
    "    fig, axes = plt.subplots(len(layer_activations), 2, \n",
    "                            figsize=(12, 4*len(layer_activations)))\n",
    "    \n",
    "    if len(layer_activations) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    channel_names = ['X', 'Y', 'Z']\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for layer_idx, (layer_name, samples_dict) in enumerate(layer_activations.items()):\n",
    "        # Get first concept's first sample\n",
    "        concept_id = list(samples_dict.keys())[0]\n",
    "        samples, heatmaps = samples_dict[concept_id]\n",
    "        \n",
    "        sample1 = samples[0]\n",
    "        sample2 = samples[1] if len(samples) > 1 else samples[0]\n",
    "        \n",
    "        # Plot first maximizing sample\n",
    "        ax1 = axes[layer_idx, 0]\n",
    "        for ch in range(3):\n",
    "            ax1.plot(sample1[ch, :].numpy(), color=colors[ch], \n",
    "                    alpha=0.7, label=channel_names[ch] if layer_idx == 0 else \"\")\n",
    "        ax1.set_title(f'{layer_name} - Concept {concept_id} - Sample 1')\n",
    "        ax1.set_ylabel('Amplitude')\n",
    "        if layer_idx == 0:\n",
    "            ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot second maximizing sample\n",
    "        ax2 = axes[layer_idx, 1]\n",
    "        for ch in range(3):\n",
    "            ax2.plot(sample2[ch, :].numpy(), color=colors[ch], alpha=0.7)\n",
    "        ax2.set_title(f'{layer_name} - Concept {concept_id} - Sample 2')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        if layer_idx == len(layer_activations) - 1:\n",
    "            ax1.set_xlabel('Time Steps')\n",
    "            ax2.set_xlabel('Time Steps')\n",
    "    \n",
    "    plt.suptitle('Multi-Layer Concept Comparison (First Concept)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"No layer activations available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Concept Evolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how concepts change across layers\n",
    "def analyze_concept_complexity(layer_activations):\n",
    "    \"\"\"Analyze the complexity of patterns captured by different layers\"\"\"\n",
    "    complexity_metrics = {}\n",
    "    \n",
    "    for layer_name, samples_dict in layer_activations.items():\n",
    "        concept_id = list(samples_dict.keys())[0]\n",
    "        samples, _ = samples_dict[concept_id]\n",
    "        \n",
    "        # Calculate various complexity metrics\n",
    "        metrics = []\n",
    "        for sample in samples[:3]:  # Analyze first 3 samples\n",
    "            # Variance across channels\n",
    "            channel_var = torch.var(sample, dim=1).mean().item()\n",
    "            \n",
    "            # Temporal variance (how much signal changes over time)\n",
    "            temporal_var = torch.var(sample, dim=0).mean().item()\n",
    "            \n",
    "            # Cross-channel correlation\n",
    "            corr_xy = torch.corrcoef(torch.stack([sample[0], sample[1]]))[0, 1].item()\n",
    "            corr_xz = torch.corrcoef(torch.stack([sample[0], sample[2]]))[0, 1].item()\n",
    "            corr_yz = torch.corrcoef(torch.stack([sample[1], sample[2]]))[0, 1].item()\n",
    "            avg_corr = np.nanmean([corr_xy, corr_xz, corr_yz])\n",
    "            \n",
    "            metrics.append({\n",
    "                'channel_var': channel_var,\n",
    "                'temporal_var': temporal_var,\n",
    "                'avg_correlation': avg_corr if not np.isnan(avg_corr) else 0\n",
    "            })\n",
    "        \n",
    "        # Average metrics across samples\n",
    "        complexity_metrics[layer_name] = {\n",
    "            'channel_var': np.mean([m['channel_var'] for m in metrics]),\n",
    "            'temporal_var': np.mean([m['temporal_var'] for m in metrics]),\n",
    "            'avg_correlation': np.mean([m['avg_correlation'] for m in metrics])\n",
    "        }\n",
    "    \n",
    "    return complexity_metrics\n",
    "\n",
    "